{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запуск spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "spark = SparkSession.builder.master('local[6]').appName('twitter_analyze').config('spark.rpc.message.maxSize','512').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#чистка от \"мусора\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "\n",
    "def clear(mas):\n",
    "    mas = [word.lower() for word in mas if word.lower() not in stop]\n",
    "    return mas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#считывание данных и токенизация\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "pattern = [\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    r'(?:@[\\w_]+)',\n",
    "    r'http[s]?://[\\S]+',\n",
    "    r'(?:[0-9]+[.,]?[0-9]+)',\n",
    "    r\"(?:[\\w][\\w’'\\-_]*[\\w]*)\"\n",
    "]\n",
    "comp = re.compile('('+'|'.join(pattern)+')', re.VERBOSE)\n",
    "\n",
    "def tokenize(text):\n",
    "    return comp.findall(text)\n",
    "\n",
    "tweet_num = 1\n",
    "    \n",
    "def update(mas):\n",
    "    global tweet_num\n",
    "    newmas = []\n",
    "    mas = clear(mas)\n",
    "    for word in mas:\n",
    "        newmas += [(word, tweet_num)]\n",
    "    tweet_num += 1\n",
    "    return newmas\n",
    "    \n",
    "    \n",
    "def get_tweets(json_file):\n",
    "    tweets = []\n",
    "    with json_file:\n",
    "        for line in json_file:\n",
    "            tweets += update(tokenize(json.loads(line).get('full_text').replace('\\n', ' ')))\n",
    "    return tweets\n",
    "\n",
    "mas = []\n",
    "with open(\"/home/yury/datasets/data1.jsonl\", encoding='utf-8') as json_file:\n",
    "    mas += get_tweets(json_file)\n",
    "with open(\"/home/yury/datasets/data2.jsonl\", encoding='utf-8') as json_file:\n",
    "    mas += get_tweets(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Перевод в DataFrame\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([ \\\n",
    "    StructField(\"word\",StringType(),True), \\\n",
    "    StructField(\"tweet_id\",IntegerType(),True), \\\n",
    "  ])\n",
    "df = spark.createDataFrame(data=mas, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13984381"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Всего слов\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подсчет количества вхождений каждого слова\n",
    "rdd2 = df.rdd.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "columns = ['word', 'count']\n",
    "df2 = rdd2.toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       word| count|\n",
      "+-----------+------+\n",
      "|   covid-19|390634|\n",
      "|      covid|133049|\n",
      "|     corona|101155|\n",
      "|        amp| 71386|\n",
      "|     people| 69516|\n",
      "|      virus| 65048|\n",
      "|   #covid19| 58684|\n",
      "|        new| 52255|\n",
      "|        get| 51151|\n",
      "|         us| 48426|\n",
      "|   pandemic| 46581|\n",
      "|coronavirus| 46530|\n",
      "|      cases| 42953|\n",
      "|       like| 41236|\n",
      "|        one| 38297|\n",
      "|     health| 37655|\n",
      "|       help| 36999|\n",
      "|       time| 35011|\n",
      "|      trump| 32763|\n",
      "|    vaccine| 32128|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Топ 10 самых популярных слов\n",
    "from pyspark.sql.functions import col, desc\n",
    "df2.sort(desc(\"count\")).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|              word|count|\n",
      "+------------------+-----+\n",
      "|          #covid19|58684|\n",
      "|      #coronavirus|30278|\n",
      "|         #covid_19|25701|\n",
      "|          #moderna|23275|\n",
      "|            #covid|18995|\n",
      "|          #covaxin|14456|\n",
      "|         #sputnikv|12182|\n",
      "|         #covid-19|10657|\n",
      "|          #vaccine|10442|\n",
      "|           #corona| 8088|\n",
      "|   #pfizerbiontech| 7566|\n",
      "|           #pfizer| 6796|\n",
      "|     #covidvaccine| 5848|\n",
      "|         #pandemic| 4240|\n",
      "|#oxfordastrazeneca| 3949|\n",
      "|          #sinovac| 3830|\n",
      "|        #covidー19| 3790|\n",
      "|      #astrazeneca| 3711|\n",
      "|         #stayhome| 3476|\n",
      "|        #sinopharm| 3414|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Топ 20 самых популярных хэштегов\n",
    "df2.filter(df2['word'].substr(1, 1) == \"#\").sort(desc('count')).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|    #moderna|23275|\n",
      "|   #sputnikv|12182|\n",
      "|     #pfizer| 6796|\n",
      "|    #sinovac| 3830|\n",
      "|#astrazeneca| 3711|\n",
      "|     moderna| 2429|\n",
      "|      pfizer| 1498|\n",
      "| astrazeneca|  736|\n",
      "|     sinovac|  475|\n",
      "|    sputnikv|   79|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Как часто встречаются названия популярных вакцин\n",
    "myfilter = df2['word'].isin(['pfizer', '#pfizer', 'sputnikv', '#sputnikv', 'moderna', '#moderna', 'astrazeneca', '#astrazeneca', 'sinovac', '#sinovac'])\n",
    "df2.filter(myfilter).sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Анализ семантики\n",
    "positive_words = {}\n",
    "negative_words = {}\n",
    "with open(\"/home/yury/datasets/positive-words.txt\", 'r') as o:\n",
    "    pos_words = o.read().splitlines()\n",
    "for pos_word in pos_words:\n",
    "    positive_words[pos_word] = 1\n",
    "with open(\"/home/yury/datasets/negative-words.txt\", 'r', encoding = \"ISO-8859-1\") as o:\n",
    "    neg_words = o.read().splitlines()\n",
    "for neg_word in neg_words:\n",
    "    negative_words[neg_word] = 1\n",
    "\n",
    "emotionpoints = [0 for i in range(tweet_num + 1)]\n",
    "for i in range(1, tweet_num):\n",
    "    filter = df.filter(df.tweet_id == i)\n",
    "    sum = 0\n",
    "    for data in filter.collect():\n",
    "        if positive_words.get(data['word']) == 1:\n",
    "            sum += 1\n",
    "        elif negative_words.get(data['word']) == 1:\n",
    "            sum -= 1\n",
    "    emotionpoints[i] = (sum / filter.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#График распределения настроений\n",
    "index = -1.0\n",
    "y_axis = []\n",
    "x_axis = []\n",
    "while index <= 1:\n",
    "    x_axis += index\n",
    "    for emotion in emotions:\n",
    "        if emotion > index - 0.01 and emotion <= index:\n",
    "            y_axis[id] += 1\n",
    "    index += 0.01\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
